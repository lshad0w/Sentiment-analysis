# -*- coding: utf-8 -*-
"""Sentiment_analysis_Lingad.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-IMWyfZE_wEoX-ud3viuV3jGv-8uNYHG
"""

!pip install contractions
!pip install emoji

import pandas as pd
import numpy as np
import re
import nltk
import emoji
import contractions
from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from imblearn.combine import SMOTETomek
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

nltk.download('punkt_tab')
nltk.download('punkt')
nltk.download('stopwords')

df = pd.read_csv('/content/spotify_reviews.csv')
df

def stars_to_sentiment(stars):
    if stars <= 2:
        return 'negative'
    elif stars == 3:
        return 'neutral'
    else:
        return 'positive'
df['sentiment'] = df['score'].apply(stars_to_sentiment)
df

import nltk
nltk.download('wordnet')
def preprocess_text(text):
    text = str(text).lower()  # Convert to lowercase
    text = contractions.fix(text)  # Expand contractions (e.g., "can't" â†’ "cannot")
    text = emoji.demojize(text)  # Convert emojis to text (e.g., "ðŸ˜Š" â†’ ":smiling_face_with_smiling_eyes:")
    text = re.sub(r"https?://\S+|www\.\S+", "", text)  # Remove URLs
    text = re.sub(r"\d+", "", text)  # Remove numbers
    text = re.sub(r"[^\w\s]", "", text)  # Remove punctuation

    tokens = word_tokenize(text)  # Tokenize words
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # Remove stopwords & lemmatize

    return ' '.join(tokens)

# Apply preprocessing
df['processed_content'] = df['content'].apply(preprocess_text)
print(df[['content', 'processed_content']].head())

df_reviews = df[['processed_content', 'score', 'sentiment']]
df_reviews

#check individual amount of [positive,neutral,negative]
print("Negative sentiment count:", (df['sentiment'] == "positive").sum())

#check data set contents and amount
print("Full dataset sentiment counts:")
print(df['sentiment'].value_counts())

print("\nTraining set sentiment counts:")
print(y_train.value_counts())

print("\nTest set sentiment counts:")
print(y_test.value_counts())



# Step 1: Import Libraries (start of trial run)
import pandas as pd
import numpy as np
from sklearn.model_selection import GridSearchCV
from collections import Counter
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MaxAbsScaler   #from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import RandomizedSearchCV

# Convert sentiment labels to numerical format
sentiment_mapping = {"positive": 2, "neutral": 1, "negative": 0}
df["sentiment"] = df["sentiment"].map(sentiment_mapping)

# Verify conversion
print(df["sentiment"].unique())  # Should print: [2, 1, 0]

# Step 2: TF-IDF Vectorization
vectorizer = TfidfVectorizer(max_features=40000, ngram_range=(1,2),sublinear_tf=True)
X = vectorizer.fit_transform(df["processed_content"])  # Convert text to numerical form
y = df["sentiment"]  # Target variable
print("TF-IDF shape:", X.shape)  # Check transformed data shape

# Step 3: Balance Dataset using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Check after applying SMOTE
print("Class distribution after SMOTE:", Counter(y_resampled))

#option
smote_tomek = SMOTETomek(random_state=42)
X_resampled, y_resampled = smote_tomek.fit_resample(X, y)

# Check distribution after balancing
print("Class distribution after SMOTE-Tomek:", Counter(y_resampled))

# Step 4: Split Data into Train & Test
# Splitting the dataset (75% train, 25% test)
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.25, random_state=42)

print("Training set size:", X_train.shape)
print("Testing set size:", X_test.shape)

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# Define hyperparameter grid
param_grid = {
    "C": np.logspace(-4, 4, 50),  # Search for best C
    "penalty": ["l2"],  # L2 regularization (newton-cg does not support L1)
}

# Define GridSearchCV
grid_search = GridSearchCV(
    estimator=LogisticRegression(random_state=42, solver="newton-cg", max_iter=2000),
    param_grid=param_grid,
    scoring="precision_macro",  # Use precision for multiclass classification
    cv=10,  # 10-fold cross-validation
    n_jobs=-1,  # Use all CPU cores
    verbose=1,  # Show progress
)

# Fit Grid Search
grid_search.fit(X_train, y_train)

# Extract best hyperparameters
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print(f"Best Precision: {best_score * 100:.2f}%")
print("Best Parameters:", best_params)



# Step 6: Train Logistic Regression Model
best_logistic = LogisticRegression(
    C=best_params["C"],
    penalty=best_params.get("penalty", "l2"),
    max_iter=2000,
    solver="newton-cg",
    random_state=42
)

# Train the model with optimized parameters
best_logistic.fit(X_train, y_train)

print("Model training complete with Bayesian Optimization parameters!")

# Step 7: Make Predictions & Evaluate (end of trial run)
# Making predictions
#y_pred = best_pipeline.predict(X_test)  # Scaling applied automatically


y_pred = best_logistic.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {accuracy * 100:.2f}%")
print("Classification Report:\n", classification_report(y_test, y_pred))

#Step 6 confusion matrix to visualize the result
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
import seaborn as sns

cm = confusion_matrix(y_test, y_pred)

sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])

def predict_sentiment(review):
    processed_review = preprocess_text(review)  # Apply same preprocessing
    review_tfidf = vectorizer.transform([processed_review])  # Convert to TF-IDF
    prediction = best_logistic.predict(review_tfidf)  # Get prediction
    sentiment_label = {2: "Positive", 1: "Neutral", 0: "Negative"}[prediction[0]]  # Convert back to label
    return sentiment_label

# Example:
print(predict_sentiment("This is the best app Iâ€™ve ever used, but it still lacks features."))
print(predict_sentiment("The interface is good, but the performance is awful..."))

import joblib
joblib.dump("/content/")